{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/suraj/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/suraj/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Read in the filenames from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 1 - The Philosopher's Stone_djvu.txt\", '/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 2 - The Chamber of Secrets_djvu.txt', '/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 3 - The Prisoner of Azkaban_djvu.txt', '/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 4 - The Goblet of Fire_djvu.txt', '/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 5 - The Order of the Phoenix_djvu.txt', '/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 6 - The Half Blood Prince_djvu.txt', '/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 7 - The Deathly Hallows_djvu.txt']\n"
     ]
    }
   ],
   "source": [
    "book_filenames = sorted(glob.glob('/home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/*.txt'))\n",
    "print(book_filenames)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Add Books Content to the Word Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in book /home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 1 - The Philosopher's Stone_djvu.txt\nAdded book to corpus, Corpus Length: 474429\nReading in book /home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 2 - The Chamber of Secrets_djvu.txt\nAdded book to corpus, Corpus Length: 1006137\nReading in book /home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 3 - The Prisoner of Azkaban_djvu.txt\nAdded book to corpus, Corpus Length: 1683115\nReading in book /home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 4 - The Goblet of Fire_djvu.txt\nAdded book to corpus, Corpus Length: 2870365\nReading in book /home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 5 - The Order of the Phoenix_djvu.txt\nAdded book to corpus, Corpus Length: 4479128\nReading in book /home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 6 - The Half Blood Prince_djvu.txt\nAdded book to corpus, Corpus Length: 5538150\nReading in book /home/suraj/Repositories/Deep Learning/Word Vectors/Harry Potter word vectors/books/Book 7 - The Deathly Hallows_djvu.txt\nAdded book to corpus, Corpus Length: 6765174\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = u''\n",
    "\n",
    "for file in book_filenames:\n",
    "    print(\"Reading in book {0}\".format(file))\n",
    "    with codecs.open(file, 'r', encoding='utf-8') as book_file:\n",
    "        raw_corpus += book_file.read()\n",
    "    print(\"Added book to corpus, Corpus Length: {0}\".format(len(raw_corpus)))"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Create NLTK Tokenizer to get sentences from the corpus. Punkt is a sentence Tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sentences and convert to words\n",
    "def sentence_to_words(sentence):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param sentence: Input sentences \n",
    "    :return: Return a list of words for the sentence\n",
    "    \"\"\"\n",
    "    cleaned_sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    words = cleaned_sentence.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63914\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I know he will.” \n\nThe scar had not pained Harry for nineteen years.\n[u'I', u'know', u'he', u'will', u'The', u'scar', u'had', u'not', u'pained', u'Harry', u'for', u'nineteen', u'years']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[63911])\n",
    "print(sentence_to_words(raw_sentences[63911]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentences) > 0:\n",
    "        sentence.append(sentence_to_words(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'THE', u'BOY', u'WHO', u'LIVED', u'Mr', u'and', u'Mrs', u'Dursley', u'of', u'number', u'four', u'Privet', u'Drive', u'were', u'proud', u'to', u'say', u'that', u'they', u'were', u'perfectly', u'normal', u'thank', u'you', u'very', u'much'], [u'They', u'were', u'the', u'last', u'people', u'you', u'd', u'expect', u'to', u'be', u'involved', u'in', u'anything', u'strange', u'or', u'mysterious', u'because', u'they', u'just', u'didn', u't', u'hold', u'with', u'such', u'nonsense'], [u'Mr', u'Dursley', u'was', u'the', u'director', u'of', u'a', u'firm', u'called', u'Grunnings', u'which', u'made', u'drills'], [u'He', u'was', u'a', u'big', u'beefy', u'man', u'with', u'hardly', u'any', u'neck', u'although', u'he', u'did', u'have', u'a', u'very', u'large', u'mustache'], [u'Mrs', u'Dursley', u'was', u'thin', u'and', u'blonde', u'and', u'had', u'nearly', u'twice', u'the', u'usual', u'amount', u'of', u'neck', u'which', u'came', u'in', u'very', u'useful', u'as', u'she', u'spent', u'so', u'much', u'of', u'her', u'time', u'craning', u'over', u'garden', u'fences', u'spying', u'on', u'the', u'neighbors'], [u'The', u'Dursley', u's', u'had', u'a', u'small', u'son', u'called', u'Dudley', u'and', u'in', u'their', u'opinion', u'there', u'was', u'no', u'finer', u'boy', u'anywhere'], [u'The', u'Dursleys', u'had', u'everything', u'they', u'wanted', u'but', u'they', u'also', u'had', u'a', u'secret', u'and', u'their', u'greatest', u'fear', u'was', u'that', u'somebody', u'would', u'discover', u'it'], [u'They', u'didn', u't', u'think', u'they', u'could', u'bear', u'it', u'if', u'anyone', u'found', u'out', u'about', u'the', u'Potters'], [u'Mrs', u'Potter', u'was', u'Mrs', u'Dursley', u's', u'sister', u'but', u'they', u'hadn', u't', u'Page', u'Harry', u'Potter', u'and', u'the', u'Philosophers', u'Stone', u'J', u'K', u'Rowling', u'met', u'for', u'several', u'years', u'in', u'fact', u'Mrs', u'Dursley', u'pretended', u'she', u'didn', u't', u'have', u'a', u'sister', u'because', u'her', u'sister', u'and', u'her', u'good', u'for', u'nothing', u'husband', u'were', u'as', u'unDursleyish', u'as', u'it', u'was', u'possible', u'to', u'be'], [u'The', u'Dursleys', u'shuddered', u'to', u'think', u'what', u'the', u'neighbors', u'would', u'say', u'if', u'the', u'Potters', u'arrived', u'in', u'the', u'street']]\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 tokenized sentences\n",
    "print(sentence[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Harry Potter Book Corpus has 1,174,677 tokens \n"
     ]
    }
   ],
   "source": [
    "# Get total number of tokens\n",
    "tokens_len = sum([len(s) for s in sentence])\n",
    "print(\"The Harry Potter Book Corpus has {0:,} tokens \".format(tokens_len))"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}